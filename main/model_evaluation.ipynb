{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model-evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wZ6tKRgx4Dy1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az3aaLX84pfK",
        "outputId": "fb5ccdce-f429-4098-f955-fd8c8a5906b7"
      },
      "source": [
        "!pip install farasapy\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from time import process_time\r\n",
        "import pickle\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "from custom_functions import *\r\n",
        "from custom_models import *\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import Sequential, Model\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from tensorflow.keras.layers import Layer\r\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, GlobalMaxPooling1D, MaxPooling1D, Conv1D, Dropout, GlobalAveragePooling1D,LSTM, Bidirectional, TimeDistributed, Flatten\r\n",
        "from tensorflow.keras.layers import concatenate\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.6/dist-packages (0.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farasapy) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2020.12.5)\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6rjVwS44Dyw"
      },
      "source": [
        "# For windows users (If exception in plot_model() function)\n",
        "# import os\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RmSYf-k4Dyw"
      },
      "source": [
        "# Data prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk3v7niE4Dyy"
      },
      "source": [
        "3.  Split into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0BOuRcE4Dyy"
      },
      "source": [
        "# D_train, D_val =  train_test_split(data ,test_size=0.2, random_state=42, stratify )\n",
        "D_train = pd.read_csv(\"../Arabic Sentiment Identification/dataset/experiments/stratified_train_set.csv\")\n",
        "D_val = pd.read_csv(\"../Arabic Sentiment Identification/dataset/experiments/stratified_val_set.csv\")\n",
        "\n",
        "D_train = D_train.dropna()\n",
        "D_val  = D_val.dropna()\n",
        "D_train = D_train.reset_index()\n",
        "D_val = D_val.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Sb6OPE4Dyy"
      },
      "source": [
        "4. Encode target variables (labels) to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxxWhPBg4Dyz",
        "outputId": "75748b80-7cb4-4010-85be-db06fe1f55bb"
      },
      "source": [
        "class_label = \"sentiment\"\n",
        "\n",
        "y_train, y_val = D_train[class_label].values.tolist(), D_val[class_label].values.tolist()\n",
        "\n",
        "y_train = get_label_encoding(y_train)\n",
        "y_val = get_label_encoding(y_val)\n",
        "\n",
        "print(y_train.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mapping:\n",
            "{'NEG': 0, 'NEU': 1, 'POS': 2}\n",
            "Mapping:\n",
            "{'NEG': 0, 'NEU': 1, 'POS': 2}\n",
            "(10029, 1) (2508, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ON7YKQ4Dyz"
      },
      "source": [
        "# Load Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw1A7IeE4Dyz"
      },
      "source": [
        "1. Tokenize Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iJW5eKtTSjO",
        "outputId": "5208d43a-644a-45e2-97b1-448a312d52d0"
      },
      "source": [
        "text_label = \"tweet_preprocessed\"\n",
        "\n",
        "# Tokenize Tweets\n",
        "x_train = D_train[text_label].values.tolist()\n",
        "x_val = D_val[text_label].values.tolist()\n",
        "corpus  = x_train + x_test\n",
        "\n",
        "tokenizer, x_train_tokenized, x_val_tokenized = tokenize_text(corpus,\n",
        "                                                               x_train, x_val)\n",
        "\n",
        "print('x_train_tokenized:',len(x_train_tokenized),'\\nx_val_tokenized:',len(x_val_tokenized),\n",
        "      \"\\nTotal Vocab: \",len(tokenizer.word_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_tokenized: 10029 \n",
            "x_val_tokenized: 2508 \n",
            "Total Vocab:  48543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcRIi_g24Dy0"
      },
      "source": [
        "2. Pad tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCAF9VW9qiy9",
        "outputId": "ca235bd3-b181-4d57-fec0-c75359a9c52a"
      },
      "source": [
        "# Pad Tweets\n",
        "maximum_text_length = 50\n",
        "padding_type = \"post\"\n",
        "truncating_type = \"post\"\n",
        "\n",
        "x_train_padded = pad_text_sequence(x_train_tokenized, maximum_text_length, padding_type, truncating_type)\n",
        "x_val_padded = pad_text_sequence(x_val_tokenized, maximum_text_length, padding_type, truncating_type)\n",
        "\n",
        "print('x_train_padded:',x_train_padded.shape,'\\nx_val_padded',x_val_padded.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_padded: (10029, 50) \n",
            "x_val_padded (2508, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iOAKIzK4Dy0"
      },
      "source": [
        "3. Generate embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUo1CmFqVGUz",
        "outputId": "ed80371e-a950-44be-b8b4-0b2d4d170d20"
      },
      "source": [
        "# Load word embeddings from file\n",
        "vocab = tokenizer.word_index\n",
        "embedding_dimension = 300\n",
        "embedding_dict_file = \"../Arabic Sentiment Identification/Word Embeddings/mazajak_pretrained_300.pkl\"\n",
        "\n",
        "embedding_matrix = get_embedding_matrix(vocab, embedding_dict_file = embedding_dict_file, embedding_dimension = embedding_dimension)\n",
        "\n",
        "# Validate embedding_matrix shape\n",
        "print(\"\\nTotal Vocab:\",len(vocab), \"\\nEmbeddings:\",embedding_matrix.shape[0] -1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading embeddings from:  ../Arabic Sentiment Identification/word embeddings/fasttext_pretrained_300.pkl\n",
            "\n",
            "Total words processed: 34722\n",
            "Words not found:  13823\n",
            "\n",
            "Total Vocab: 48544 \n",
            "Embeddings: 48544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ6tKRgx4Dy1"
      },
      "source": [
        "# Load Sentiment Features "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYB-bdrs4Dy1"
      },
      "source": [
        "def get_sentiment_matrix(df):\n",
        "    sentiment_matrix  = np.zeros((len(df), 5), dtype='float64')\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        try:\n",
        "            sentiment_matrix[i] = np.array([df['NEU_WEIGHTS'][i],df['POSNEG_WEIGHTS'][i], df['Pos_P'][i],\n",
        "                                            df['Neg_P'][i], df['Neu_P'][i]], dtype='float64')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    return sentiment_matrix\n",
        "\n",
        "def get_tweet_weights(corpus, weights_dict):\n",
        "  neu_weights = []\n",
        "  posneg_weights = []\n",
        "  for tweet in corpus:\n",
        "    tweet = str(tweet)\n",
        "    neu_c, posneg_c = 0,0\n",
        "    for word in tweet.split():\n",
        "      if word in weights_dict.keys():\n",
        "        neu_c += weights_dict.get(word)[0]\n",
        "        posneg_c += weights_dict.get(word)[1]\n",
        "      else:\n",
        "        neu_c += 0\n",
        "        posneg_c += 0\n",
        "    neu_weights.append(neu_c)\n",
        "    posneg_weights.append(posneg_c)\n",
        "  return neu_weights, posneg_weights\n",
        "\n",
        "def get_tweet_probabilities(corpus, word_probabilities):\n",
        "  Pos_P = []\n",
        "  Neg_P = []\n",
        "  Neu_P = []\n",
        "  for tweet in corpus:\n",
        "    tweet = str(tweet)\n",
        "    pos, neg, neu = 0,0,0\n",
        "    for word in tweet.split():\n",
        "      p = word_probabilities.get((word, 'POS'))\n",
        "      if p!=None:\n",
        "        pos += p\n",
        "      n = word_probabilities.get((word, 'NEG'))\n",
        "      if n!=None:\n",
        "        neg += n\n",
        "      n1 = word_probabilities.get((word, 'NEU'))\n",
        "      if n1!=None:\n",
        "        neu += n1\n",
        "    Pos_P.append(pos)\n",
        "    Neg_P.append(neg)\n",
        "    Neu_P.append(neu)\n",
        "  return Pos_P, Neg_P, Neu_P\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwn-sf4M4Dy1",
        "outputId": "23854b97-69a2-410c-b879-7e5c092d080b"
      },
      "source": [
        "weights_dict = load_from_pickle(\"../Arabic Sentiment Identification/dataset/experiments/word_probabilities.pkl\")\n",
        "word_probabilities = load_from_pickle(\"../Arabic Sentiment Identification/dataset/experiments/word_weights.pkl\")\n",
        "\n",
        "# Getting x-train features\n",
        "neu_weights, posneg_weights = get_tweet_weights(x_train, weights_dict)\n",
        "Pos_P, Neg_P, Neu_P = get_tweet_probabilities(x_train, word_probabilities)\n",
        "\n",
        "D_train[\"NEU_WEIGHTS\"] = neu_weights\n",
        "D_train['POSNEG_WEIGHTS'] = posneg_weights\n",
        "D_train['Neg_P'] = Neg_P \n",
        "D_train['Pos_P'] = Pos_P\n",
        "D_train['Neu_P'] = Neu_P\n",
        "\n",
        "x_train_sentiment_matrix = get_sentiment_matrix(D_train)\n",
        "print(\"Sentiment Matrix (Training): \",x_train_sentiment_matrix.shape)\n",
        "\n",
        "# Getting x-train features\n",
        "neu_weights, posneg_weights = get_tweet_weights(x_val, weights_dict)\n",
        "Pos_P, Neg_P, Neu_P = get_tweet_probabilities(x_val, word_probabilities)\n",
        "\n",
        "D_val[\"NEU_WEIGHTS\"] = neu_weights\n",
        "D_val['POSNEG_WEIGHTS'] = posneg_weights\n",
        "D_val['Neg_P'] = Neg_P \n",
        "D_val['Pos_P'] = Pos_P\n",
        "D_val['Neu_P'] = Neu_P\n",
        "\n",
        "x_val_sentiment_matrix = get_sentiment_matrix(D_val)\n",
        "\n",
        "print(\"Sentiment Matrix (Val): \",x_val_sentiment_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentiment matrix (Train): (10029, 4)\n",
            "Sentiment matrix (Test): (2508, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vta-ffo85cLH"
      },
      "source": [
        "# CNN baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2zfOkJU5aV3"
      },
      "source": [
        "# Define callback\r\n",
        "custom_callback = myCallbacks(metrics=\"acc\", threshold = 0.95)\r\n",
        "\r\n",
        "# Define model hyperparameters\r\n",
        "input_length = maximum_text_length\r\n",
        "embedding_vocab = embedding_matrix.shape[0]\r\n",
        "embedding_dimension = embedding_matrix.shape[1]\r\n",
        "output_dimension = len(np.unique(y_train))\r\n",
        "\r\n",
        "print('Shape of each Input Sentence: ',input_length,\"x\",embedding_dimension)\r\n",
        "print('Shape of Input layer: ',len(x_train),\"x\",embedding_dimension)\r\n",
        "print(\"Output classes: \",output_dimension)\r\n",
        "\r\n",
        "# Load model\r\n",
        "CNN_model = CNN(input_length = input_length, input_dimension = embedding_vocab, \r\n",
        "                  embedding_dimension = embedding_dimension, output_dimension = output_dimension,\r\n",
        "                  embedding_matrix = embedding_matrix, num_layers = 1, trainable = False,\r\n",
        "                   kernel_size = 5, dropout_rate = 0.25)\r\n",
        "\r\n",
        "# View model summary\r\n",
        "# print(\"\\n\\nModel Summary:\")\r\n",
        "# CNN_model.summary()\r\n",
        "\r\n",
        "# Train model\r\n",
        "print(\"\\n\\nTraining Model:\")\r\n",
        "model_history = CNN_model.fit(x = np.asarray(x_train_padded), y = np.asarray(y_train),\r\n",
        "                               validation_data = (np.asarray(x_val_padded),np.asarray(y_val)),\r\n",
        "                               epochs = 10, callbacks = [custom_callback])\r\n",
        "\r\n",
        "# plot results\r\n",
        "plot_results(model_history)\r\n",
        "\r\n",
        "# print classification report\r\n",
        "y_pred = np.argmax(model.predict([x_val_padded]),axis=-1)\r\n",
        "print(\"\\nClassification Report:\\n\\n\",classification_report(y_true=y_val, y_pred = y_pred, labels = np.unique(y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJeErdFf5rfI"
      },
      "source": [
        "# BiLSTM baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VdhOyKM5tfj"
      },
      "source": [
        "# Define callback\r\n",
        "custom_callback = myCallbacks(metrics=\"acc\", threshold = 0.95)\r\n",
        "\r\n",
        "# Define model hyperparameters\r\n",
        "input_length = maximum_text_length\r\n",
        "embedding_vocab = embedding_matrix.shape[0]\r\n",
        "embedding_dimension = embedding_matrix.shape[1]\r\n",
        "output_dimension = len(np.unique(y_train))\r\n",
        "\r\n",
        "print('Shape of each Input Sentence: ',input_length,\"x\",embedding_dimension)\r\n",
        "print('Shape of Input layer: ',len(x_train),\"x\",embedding_dimension)\r\n",
        "print(\"Output classes: \",output_dimension)\r\n",
        "\r\n",
        "# Load model\r\n",
        "BILSTM_model = BILSTM(input_length = input_length, input_dimension = embedding_vocab, \r\n",
        "                   embedding_dimension = embedding_dimension, output_dimension = output_dimension,\r\n",
        "                   embedding_matrix = embedding_matrix, layer1 = 64, layer2 = 64, trainable = False,\r\n",
        "                   dropout_rate = 0.25)\r\n",
        "\r\n",
        "# View model summary\r\n",
        "# print(\"\\n\\nModel Summary:\")\r\n",
        "# BILSTM_model.summary()\r\n",
        "\r\n",
        "# Train model\r\n",
        "print(\"\\n\\nTraining Model:\")\r\n",
        "model_history = BILSTM_model.fit(x = np.asarray(x_train_padded), y = np.asarray(y_train),\r\n",
        "                               validation_data = (np.asarray(x_val_padded),np.asarray(y_val)),\r\n",
        "                               epochs = 10, batch_size = 32, callbacks = [custom_callback])\r\n",
        "\r\n",
        "# plot results\r\n",
        "plot_results(model_history)\r\n",
        "\r\n",
        "# print classification report\r\n",
        "y_pred = np.argmax(model.predict([x_val_padded]),axis=-1)\r\n",
        "print(\"\\nClassification Report:\\n\\n\",classification_report(y_true=y_val, y_pred = y_pred, labels = np.unique(y_val)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_S_9gl_6VlD"
      },
      "source": [
        "# CNN - BiLSTM baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvZzYce6YV_"
      },
      "source": [
        "# Input \r\n",
        "input1 = embedding_matrix\r\n",
        "input_len1 = maximum_text_length\r\n",
        "embedding_vocab1 = embedding_matrix.shape[0]\r\n",
        "embedding_dimension1 = embedding_matrix.shape[1]\r\n",
        "\r\n",
        "# Output\r\n",
        "output_dim = len(np.unique(y_train))\r\n",
        "\r\n",
        "#--------------------------------------------------------------------------------------------------------------------#\r\n",
        "\r\n",
        "# Input Channel 1\r\n",
        "i1 = Input(shape = (input_len1, ))\r\n",
        "e1 = Embedding(input_length=input_len1, input_dim=embedding_vocab1, output_dim = embedding_dimension1,\r\n",
        "               weights = [input1], trainable = False)(i1)\r\n",
        "c1 = Conv1D(filters = 256, kernel_size=3, padding = \"same\", activation=\"relu\")(e1)\r\n",
        "c1_do = Dropout(0.3)(c1)\r\n",
        "max_pool1 = MaxPooling1D(pool_size=3)(c1_do)\r\n",
        "\r\n",
        "b1 = Bidirectional(LSTM(128, dropout = 0.3, return_sequences = True, ))(max_pool1)\r\n",
        "gmp1 = GlobalMaxPooling1D()(b1)\r\n",
        "\r\n",
        "d1 = Dense(64, activation = \"relu\")(gmp1)\r\n",
        "\r\n",
        "# Output layer\r\n",
        "output = Dense(output_dim, activation = \"softmax\")(d1)\r\n",
        "\r\n",
        "#---------------------------------------------------------------------------------------------------------------------#\r\n",
        "\r\n",
        "# Compile\r\n",
        "model = Model(inputs = [i1], outputs = output)\r\n",
        "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"acc\"])\r\n",
        "model_history = model.fit([x_train_padded], y_train, epochs=10, verbose = 1, batch_size=64,\r\n",
        "                         validation_data=([x_val_padded], y_val))\r\n",
        "\r\n",
        "# # plot results\r\n",
        "# plot_results(model_history)\r\n",
        "\r\n",
        "# print classification report\r\n",
        "y_pred = np.argmax(model.predict([x_val_padded]),axis=-1)\r\n",
        "print(\"\\nClassification Report:\\n\\n\",classification_report(y_true=y_val, y_pred = y_pred, labels = np.unique(y_val)))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s53CttjGxvV"
      },
      "source": [
        "# Expermiental models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5wMiF5HG1K1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}